import os
import cv2
import torch
import numpy as np
import torch.nn as nn
from torch.amp import autocast
from torchvision import transforms, datasets

from data_loaders.cifar import get_loaders
from models.vision_backbones import ResNet50Embed
from models.kg_encoder import KGEncoderGAT
from models.fusion import CrossAttentionFuse
from kg.io import load_kg_csv
from kg.build_graph import make_edge_index
from explain.gradcam import GradCAM
from explain.kg_path import KGPathExtractor


def overlay_heatmap(img_t, cam_t):
    """
    æŠŠ Grad-CAM çƒ­åŠ›å›¾å åŠ åˆ°åŸå›¾ä¸Šï¼Œè¾“å‡ºä¸€å¼ å½©è‰²å¯è¯»å›¾ã€‚
    img_t: [3,H,W] æ¨¡å‹è¾“å…¥ç”¨çš„å›¾åƒ(æ ‡å‡†åŒ–å)
    cam_t: Grad-CAM è¾“å‡º (å¯ä»¥æ˜¯[1,1,h,w] æˆ– [1,h,w] æˆ– [h,w])

    è¿”å›: uint8 çš„ (H,W,3) RGB å›¾åƒï¼Œå¯ç›´æ¥ cv2.imwrite
    """
    # 1. åå½’ä¸€åŒ–åˆ°äººçœ¼å¯è¯»
    denorm = transforms.Normalize(
        mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225],
        std=[1 / 0.229, 1 / 0.224, 1 / 0.225],
    )
    img = denorm(img_t.clone()).clamp(0, 1)  # [3,H,W]
    img_np = img.permute(1, 2, 0).cpu().numpy()  # [H,W,3] in [0,1]
    H, W = img_np.shape[:2]

    # 2. å¤„ç†CAMå½¢çŠ¶ï¼Œæ‹‰æˆ2Dçƒ­åº¦å›¾
    cam_arr = cam_t.detach().cpu().squeeze()
    cam_arr = cam_arr.numpy().astype(np.float32)
    cam_arr = cv2.resize(cam_arr, (W, H))

    # 3. å½’ä¸€åŒ–åˆ°0-255å¹¶ä¸Šä¼ªå½©è‰²
    cam_arr = cam_arr - cam_arr.min()
    cam_arr = cam_arr / (cam_arr.max() + 1e-6)
    cam_uint8 = (cam_arr * 255).astype(np.uint8)

    heat = cv2.applyColorMap(cam_uint8, cv2.COLORMAP_JET)  # BGR
    heat = cv2.cvtColor(heat, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0  # -> RGB, [0,1]

    # 4. èåˆ(0.8åŸå›¾ + 0.2çƒ­åŠ›)ï¼Œè®©ä¸»ä½“å°½é‡æ¸…æ™°
    overlay = 0.8 * img_np + 0.2 * heat
    overlay = np.clip(overlay, 0, 1)
    overlay_uint8 = (overlay * 255).astype(np.uint8)
    return overlay_uint8


if __name__ == "__main__":
    os.makedirs("./outputs", exist_ok=True)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Device (in dump_examples): {device}")

    # ============ 1. åŠ è½½æ•°æ® (éªŒè¯é›†) ============
    # æˆ‘ä»¬ç”¨è¾ƒå¤§ batch_size æ¥å°½å¿«é‡åˆ°ç›®æ ‡ç±»åˆ«
    _, val_loader, num_classes = get_loaders(
        data_root="./data",
        batch_size=128,
        num_workers=2,
        img_size=224,
    )

    # CIFAR-100 çš„ç±»åï¼ˆä¸‹æ ‡ -> ç±»åï¼‰
    tmp = datasets.CIFAR100(root="./data", train=False, download=True)
    classes = tmp.classes  # e.g. ["apple", "aquarium_fish", ..., "train", ...]
    name_to_idx = {name: idx for idx, name in enumerate(classes)}

    # æˆ‘ä»¬æƒ³é‡ç‚¹å±•ç¤º/è§£é‡Šçš„ç±»åˆ«ï¼ˆè®ºæ–‡é‡Œä¼šæ”¾å›¾çš„é‚£æ‰¹ï¼‰
    target_names = [
        #"tiger", "wolf", "seal", "whale", "fox", "dolphin"    # åŠ¨ç‰©/å“ºä¹³/æ°´ç”Ÿå“ºä¹³
        #"fox", "dolphin"                                      # åŠ¨ç‰©/å“ºä¹³/æ°´ç”Ÿå“ºä¹³ï¼ˆä½¿ç”¨ï¼‰
        #"bus", "truck", "train", "bicycle", "rocket", "tank"  # äº¤é€šå·¥å…·
        "bus", "train"                                       # äº¤é€šå·¥å…·(ä½¿ç”¨ï¼‰
        #"rose", "tulip", "pine_tree",                         # æ¤ç‰©/èŠ±/æ ‘
    ]
    # è¿‡æ»¤å‡ºCIFARä¸­çœŸå®å­˜åœ¨çš„ç±»
    target_names = [n for n in target_names if n in name_to_idx]

    # æ¯ä¸ªç±»æœ€å¤šä¿å­˜å‡ å¼ å›¾ï¼ˆé˜²æ­¢dumpå¤ªå¤šï¼‰
    max_per_class = 20
    saved_per_class = {n: 0 for n in target_names}

    # ============ 2. è½½å…¥è®­ç»ƒå¥½çš„æœ€ä¼˜æƒé‡ ============
    ckpt = torch.load("./checkpoints/kgxnn_best.pt", map_location=device)
    # kgxnn_rand_noattr.pt
    # kgxnn_rand_attr.pt
    # kgxnn_glove_noattr.pt
    # kgxnn_best.pt ï¼ˆç›¸å½“äºå°±æ˜¯ kgxnn_glove_attr.pt)

    # è§†è§‰ä¸»å¹²
    backbone = ResNet50Embed(pretrained=True).to(device)
    backbone.load_state_dict(ckpt["backbone"])

    # çŸ¥è¯†å›¾è°±ç¼–ç æ¨¡å—
    kg_enc = KGEncoderGAT(in_dim=300, hid=256, heads=4).to(device)
    kg_enc.load_state_dict(ckpt["kg_enc"])

    # Cross-Attention èåˆæ¨¡å—
    fuse = CrossAttentionFuse(v_dim=2048, k_dim=256, out_dim=256).to(device)
    fuse.load_state_dict(ckpt["fuse"])

    # KG èŠ‚ç‚¹çš„å¯è®­ç»ƒå‘é‡
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬æ¢å¤çš„æ˜¯ node_embï¼Œè€Œä¸æ˜¯é‡æ–°init
    node_emb = nn.Embedding(
        ckpt["node_emb"]["weight"].shape[0],
        ckpt["node_emb"]["weight"].shape[1],
    ).to(device)
    node_emb.load_state_dict(ckpt["node_emb"])

    # åˆ†ç±»å¤´ï¼šä» joint_feat -> CIFAR100 logits
    cls_head = nn.Linear(2048 + 256, num_classes).to(device)
    cls_head.load_state_dict(ckpt["cls_head"])

    # å±æ€§å¤´ï¼ˆè™½ç„¶è¿™è„šæœ¬é‡Œä¸ç›´æ¥ç”¨å®ƒç”»å›¾ï¼Œä½†æˆ‘ä»¬loadè¿›æ¥ä¿è¯ä¸€è‡´æ€§ï¼‰
    if "attr_head" in ckpt:
        attr_head = nn.Linear(
            2048 + 256,
            ckpt["attr_head"]["weight"].shape[0]
        ).to(device)
        attr_head.load_state_dict(ckpt["attr_head"])
    else:
        attr_head = None  # å‘åå…¼å®¹æ—§checkpoint

    # çŸ¥è¯†å›¾è°±è¾¹
    edge_index = ckpt["edge_index"].to(device)

    # è½½å…¥KGçš„ç»“æ„ï¼ˆèŠ‚ç‚¹/è¾¹ï¼‰
    nodes_df, edges_df, _ = load_kg_csv("./kg")

    # ç”¨äºäººç±»å¯è¯»è§£é‡Šï¼ˆè¯­ä¹‰å±‚çº§ + å…³é”®å±æ€§ï¼‰
    kge = KGPathExtractor(nodes_df, edges_df)

    # Grad-CAM é’©å­ï¼ŒæŒ‚åœ¨è§†è§‰ backbone çš„æœ€åä¸€å±‚å·ç§¯ï¼ˆé»˜è®¤ layer4ï¼‰
    gradcam = GradCAM(backbone)

    # ============ 3. å®šä¹‰ forward_fn ç»™ Grad-CAM è°ƒç”¨ ============
    # Grad-CAM éœ€è¦ä¸€ä¸ªâ€œæˆ‘ç»™ä½ ä¸€å¼ å›¾ -> ä½ ç®—å‡ºåˆ†ç±»logitsâ€çš„å‡½æ•°ã€‚
    # æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªå‡½æ•°å’Œè®­ç»ƒè·¯å¾„å®Œå…¨ä¸€è‡´ (v -> kg -> z -> concat -> cls_head)
    def forward_fn(x_batch):
        with autocast(device_type="cuda", enabled=(device.type == "cuda")):
            v_ = backbone(x_batch)                             # [B,2048]
            kg_nodes_ = kg_enc(node_emb.weight, edge_index)    # [N,256]
            z_, _attn_ = fuse(v_, kg_nodes_)                   # z_:[B,256]
            joint_ = torch.cat([v_, z_], dim=-1)               # [B,2304]
            logits_ = cls_head(joint_)                         # [B,num_classes]
        return logits_

    # ============ 4. éå†éªŒè¯é›†ï¼ŒæŒ‘ç›®æ ‡ç±»åˆ«çš„æ ·æœ¬å¹¶å¯¼å‡ºå¯è§†åŒ– ============
    for images, labels in val_loader:
        images = images.to(device)
        labels = labels.to(device)

        # å‰å‘ï¼šæ‹¿åˆ°åˆ†ç±»é¢„æµ‹ + æ³¨æ„åŠ› (attn) ä»¥ä¾¿æå–å…³é”®çŸ¥è¯†èŠ‚ç‚¹
        with autocast(device_type="cuda", enabled=(device.type == "cuda")):
            v = backbone(images)                               # [B,2048]
            kg_nodes_vec = kg_enc(node_emb.weight, edge_index) # [N,256]
            z, attn = fuse(v, kg_nodes_vec)                    # z:[B,256], attn:[B,N]
            joint_feat = torch.cat([v, z], dim=-1)             # [B,2304]
            logits = cls_head(joint_feat)                      # [B,num_classes]
            preds = logits.argmax(dim=1)                       # [B]

        B = images.size(0)
        for i in range(B):
            true_idx = int(labels[i].item())
            true_name = classes[true_idx]

            # åªå¯¼å‡ºæˆ‘ä»¬å…³å¿ƒçš„ç±»
            if true_name not in saved_per_class:
                continue
            if saved_per_class[true_name] >= max_per_class:
                continue

            # æ³¨é‡Šæ‰ä»¥ä¸‹ä¸‰è¡Œä»¥è·³è¿‡æ³¨æ„åŠ›çƒ­åŠ›å›¾ç”Ÿæˆ
            img_i = images[i:i+1]  # [1,3,H,W]
            cam_map = gradcam(img_i, forward_fn, target_class=preds[i:i+1])
            overlay_img = overlay_heatmap(images[i].cpu(), cam_map[0])

            # æ”¹ä¸ºç›´æ¥ä½¿ç”¨åŸå§‹å›¾åƒï¼ˆåå½’ä¸€åŒ–åï¼‰
            # denorm = transforms.Normalize(
            #     mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225],
            #     std=[1 / 0.229, 1 / 0.224, 1 / 0.225],
            # )
            # img = denorm(images[i].clone()).clamp(0, 1)  # [3,H,W]
            # img_np = img.permute(1, 2, 0).cpu().numpy()  # [H,W,3] in [0,1]
            # overlay_img = (img_np * 255).astype(np.uint8)

            # è§£é‡ŠçŸ¥è¯†æ³¨æ„åŠ›ï¼š
            # 1. å–è¯¥æ ·æœ¬çš„ cross-attention åˆ†å¸ƒ attn[i]  ( [N_nodes] )
            # 2. æ‰¾æ³¨æ„åŠ›æœ€é«˜çš„è‹¥å¹²KGèŠ‚ç‚¹ï¼Œäº¤ç»™è§£é‡Šå™¨
            attn_i = attn[i].detach().cpu()  # [N_nodes]
            topk_idx = torch.topk(attn_i, k=3).indices.numpy().tolist()

            pred_name = classes[int(preds[i].item())]

            # kge.describe ä¼šè¿”å›ç±»ä¼¼ï¼š
            # "è¯­ä¹‰å±‚çº§: pine_tree â†’ conifer â†’ tree â†’ plantï¼›å…³é”®å±æ€§: has_petals, is_conifer"
            explain_text = kge.describe(pred_name, topk_idx)

            # è¾“å‡ºæ–‡ä»¶åç¤ºä¾‹ï¼š pine_tree_0_overlay.jpg / pine_tree_0_explain.txt
            out_idx = saved_per_class[true_name]
            img_out_path = f"./outputs/{true_name}_{out_idx}_overlay.jpg"
            txt_out_path = f"./outputs/{true_name}_{out_idx}_explain.txt"

            # ä¿å­˜å åŠ çƒ­å›¾
            cv2.imwrite(
                img_out_path,
                cv2.cvtColor(overlay_img, cv2.COLOR_RGB2BGR)  # OpenCVå†™æ–‡ä»¶ç”¨BGR
            )

            # ä¿å­˜æ–‡æœ¬è§£é‡Š
            with open(txt_out_path, "w", encoding="utf-8") as f:
                f.write(f"true={true_name}\n")
                f.write(f"pred={pred_name}\n")
                f.write(explain_text + "\n")

            print(f"[Saved] {true_name} #{out_idx} -> {img_out_path}")
            saved_per_class[true_name] += 1

        # å¦‚æœæ‰€æœ‰ç›®æ ‡ç±»éƒ½å¤Ÿäº†ï¼Œå°±æå‰é€€å‡º
        if all(saved_per_class[n] >= max_per_class for n in saved_per_class):
            break

    # åœ¨ä¸»å¾ªç¯ä¹‹åå¢åŠ é€æ ·æœ¬ç»Ÿè®¡é€»è¾‘
    print("\nğŸ“Š é€æ ·æœ¬è§£é‡Šç»Ÿè®¡:")

    # åˆå§‹åŒ–ç»Ÿè®¡å˜é‡
    total_samples = 0
    explained_samples = 0
    class_sample_stats = {name: {'total': 0, 'explained': 0} for name in target_names}

    # éå†æ¯ä¸ªç›®æ ‡ç±»åˆ«å’Œæ¯ä¸ªæ ·æœ¬
    for name in target_names:
        for idx in range(max_per_class):
            txt_path = f"./outputs/{name}_{idx}_explain.txt"
            if os.path.exists(txt_path):
                # å¢åŠ è¯¥ç±»åˆ«çš„æ ·æœ¬è®¡æ•°
                class_sample_stats[name]['total'] += 1
                # å¢åŠ æ€»æ ·æœ¬è®¡æ•°
                total_samples += 1

                with open(txt_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ³¨æ„åŠ›å‘½ä¸­
                    if "æ³¨æ„åŠ›å‘½ä¸­èŠ‚ç‚¹: æ— " not in content:
                        class_sample_stats[name]['explained'] += 1
                        explained_samples += 1

    # è¾“å‡ºæ¯ä¸ªç±»åˆ«çš„è¯¦ç»†ç»Ÿè®¡
    for name, stats in class_sample_stats.items():
        if stats['total'] > 0:
            class_coverage = stats['explained'] / stats['total']
            print(f"- {name}: {stats['explained']}/{stats['total']} æ ·æœ¬è¢«è§£é‡Š ({class_coverage:.2%})")

    # è¾“å‡ºæ€»ä½“ç»Ÿè®¡
    overall_coverage = explained_samples / total_samples if total_samples > 0 else 0
    print(f"\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
    print(f"- æ€»å…±å¯¼å‡ºæ ·æœ¬æ•°: {total_samples}")
    print(f"- æˆåŠŸè§£é‡Šçš„æ ·æœ¬æ•°: {explained_samples}")
    print(f"- æ€»ä½“è§£é‡Šè¦†ç›–ç‡: {overall_coverage:.2%}")

    print("âœ… å·²å¯¼å‡ºç›®æ ‡ç±»åˆ«çš„å¯è§†åŒ–ç»“æœåˆ° ./outputs/ ç›®å½•ä¸‹ã€‚")

    # Explanation Coverage Rate:
    # è¡¡é‡æ¨¡å‹ä¸ºé¢„æµ‹æ ·æœ¬æä¾›æœ‰æ•ˆçŸ¥è¯†å›¾è°±è§£é‡Šçš„èƒ½åŠ›ï¼Œè®¡ç®—å…¬å¼ä¸ºï¼šèƒ½ç”Ÿæˆæœ‰æ•ˆKGè·¯å¾„è§£é‡Šçš„æ ·æœ¬æ•° / æ€»è¯„ä¼°æ ·æœ¬æ•°